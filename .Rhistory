check_accuracy(pred_test_3, test_labels, "Test")
check_accuracy(pred_test_2, test_labels, "Test")
check_accuracy(pred_test_4, test_labels, "Test")
check_accuracy(pred_test_5, test_labels, "Training")
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
check_accuracy(pred_test_7, test_labels, "Test")
check_accuracy(pred_test_8, test_labels, "Test")
check_accuracy(pred_test_9, test_labels, "Test")
check_accuracy(pred_test_13, test_labels, "Test")
pred_test_13 <- predict(nb_mod13, test)
check_accuracy(pred_test_13, test_labels, "Test")
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table)
poll<-as.data.frame(fread(file.choose()))
poll <- data.frame(apply(poll,2,function(x) factor(x)))
poll$vote_2008 <- relevel(poll$vote_2008, ref = "john mcCain")
levels(poll$vote_2008)
summary(glm(vote_2008~.,data = poll, family = "binomial"))
#probabilistic predictions into binary predictions
f1 <- glm(vote_2008~.,data = poll, family = "binomial")
predicted <- predict(f1, type = "response")
binarypredictions <- ifelse(predicted > .5, "Obama", "McCain")
table(binarypredictions)
#accuracy
confusion_matrix <- table(binarypredictions, poll$vote_2008)
(confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)
#precision
(5119)/(823+5119)
#recall
5119/(5119+600)
binarypredictions_new <- ifelse(predicted > .7, "Obama", "McCain")
table(binarypredictions_new)
#accuracy
confusion_matrix_new <- table(binarypredictions_new, poll$vote_2008)
(confusion_matrix_new[1,1]+confusion_matrix_new[2,2])/sum(confusion_matrix)
#precision
(4410)/(4410+398)
#recall
(4410)/(4410+1309)
#read in data
poll_full <- as.data.frame(fread(file.choose()))
#mutate with if_else to make democrat + republican =1 for major party, other = 0
poll_full$MajorParty <- ifelse(poll_full$vote_2008 == "other", 0, 1)
#factor
poll_full$MajorParty <- factor(poll_full$MajorParty)
library(MASS)
bigmodel <- glm(MajorParty~.-vote_2008, data = poll_full, family = "binomial")
# summary(stepAIC(bigmodel, scope = list(lower = ~1, upper = ~ .^2), direction = "both"))
# finalmodel <-glm(formula = MajorParty ~ sex + race + party + ideology + party:ideology +
#     race:party + sex:race + sex:party, family = "binomial", data = poll_full)
summary(finalmodel)
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=predicted2))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=prob_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
#filtering
library(tidyverse)
poll_full_filter <- poll_full %>%
filter(MajorParty == 1)%>%
mutate(vote_2008 = factor(vote_2008))%>%
select(-MajorParty)
poll_full_filter <- data.frame(apply(poll_full_filter,2,function(x) factor(x)))
poll_full_filter$vote_2008 <- relevel(poll_full_filter$vote_2008, ref = "john mcCain")
levels(poll_full_filter$vote_2008)
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Obama | Voted for Major Party")+
labs(x="probabilities")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table)
poll<-as.data.frame(fread(file.choose()))
poll <- data.frame(apply(poll,2,function(x) factor(x)))
poll$vote_2008 <- relevel(poll$vote_2008, ref = "john mcCain")
levels(poll$vote_2008)
summary(glm(vote_2008~.,data = poll, family = "binomial"))
#probabilistic predictions into binary predictions
f1 <- glm(vote_2008~.,data = poll, family = "binomial")
predicted <- predict(f1, type = "response")
binarypredictions <- ifelse(predicted > .5, "Obama", "McCain")
table(binarypredictions)
#accuracy
confusion_matrix <- table(binarypredictions, poll$vote_2008)
(confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)
#precision
(5119)/(823+5119)
#recall
5119/(5119+600)
binarypredictions_new <- ifelse(predicted > .7, "Obama", "McCain")
table(binarypredictions_new)
#accuracy
confusion_matrix_new <- table(binarypredictions_new, poll$vote_2008)
(confusion_matrix_new[1,1]+confusion_matrix_new[2,2])/sum(confusion_matrix)
#precision
(4410)/(4410+398)
#recall
(4410)/(4410+1309)
#read in data
poll_full <- as.data.frame(fread(file.choose()))
#mutate with if_else to make democrat + republican =1 for major party, other = 0
poll_full$MajorParty <- ifelse(poll_full$vote_2008 == "other", 0, 1)
#factor
poll_full$MajorParty <- factor(poll_full$MajorParty)
library(MASS)
bigmodel <- glm(MajorParty~.-vote_2008, data = poll_full, family = "binomial")
# summary(stepAIC(bigmodel, scope = list(lower = ~1, upper = ~ .^2), direction = "both"))
# finalmodel <-glm(formula = MajorParty ~ sex + race + party + ideology + party:ideology +
#     race:party + sex:race + sex:party, family = "binomial", data = poll_full)
#summary(finalmodel)
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=prob_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Major Party")+
labs(x="probabilities")
#filtering
library(tidyverse)
poll_full_filter <- poll_full %>%
filter(MajorParty == 1)%>%
mutate(vote_2008 = factor(vote_2008))%>%
select(-MajorParty)
poll_full_filter <- data.frame(apply(poll_full_filter,2,function(x) factor(x)))
poll_full_filter$vote_2008 <- relevel(poll_full_filter$vote_2008, ref = "john mcCain")
levels(poll_full_filter$vote_2008)
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Obama | Voted for Major Party")+
labs(x="probabilities")
#Prob(Obama) = Prob(Obama|Voted for Major Party)*Prob(Major Party)
prob_obama <- prob_obama_given_major_party*prob_major_party
#Prob(McCain) = Prob(McCain|Voted for Major Party)*Prob(Major Party)
#=(1- Prob(Obama|Voted for Major Party)*Prob(Major Party)
prob_mccain <- (1 - prob_obama_given_major_party)*prob_major_party
#Prob(Other) = 1- Prob(Major Party)
prob_other <- (1- prob_major_party)
#Check that everything adds up to 1
mean(round(prob_obama+prob_mccain+prob_other, 4) == 1)
#Bind Probabilities into Dataframe
probabilities <- data.frame(prob_obama = prob_obama,
prob_mccain = prob_mccain,
prob_other = prob_other)
#Generate new variable of predicted vote for each individual
#Predicted vote is based on highest probability
probabilities <- probabilities %>%
mutate(predicted_vote = as.factor(case_when(
prob_obama > prob_mccain &  prob_obama > prob_other ~ "barack obama",
prob_mccain > prob_obama & prob_mccain > prob_other ~ "john mcCain",
prob_other > prob_obama & prob_other > prob_mccain ~ "other"
)))
#Add "other" to factor levels since there were no "other" predictions
levels(probabilities$predicted_vote) <- c("barack obama", "john mcCain", "other")
#Explore Vote Predictions
table(probabilities$predicted_vote)
#Confusion Matrix
confusion_mat_3 <- table(probabilities$predicted_vote, poll_full$vote_2008)
confusion_mat_3
#Compute Accuracy
cat("\n Accuracy: ", sum(diag(confusion_mat_3)) / sum(confusion_mat_3))
#84% accuracy; not too bad
>>>>>>> 1a5fa08b3dd696e375ac30c75bea8dcb1a7081d8
#Loading necessary packages
library(data.table)
library(lubridate)
library(dplyr)
library(tidytext) #may have to download this if you do not have it
library(stringr)
library(naivebayes)
library(ggplot2)
#Read in Data
trump <- as.data.frame(fread("trump_data.tsv", header = FALSE))
names(trump) <- c("source", "time_posted", "text")
#Clean time
trump$time_posted <- gsub("[^-0-9//:]", " ", trump$time_posted)
trump$time_posted <-trimws(trump$time_posted)
trump$time_posted <- ymd_hms(trump$time_posted)
##### Part 1: Cleaning/ Features #####
#Feature 1: Hour of Day
trump$hour <- hour(trump$time_posted)
#Important Note - if 1- means has feature, if 0 - means does not have feature!
#Feature 2: Start with Quotation Mark
trump$quote <- ifelse(grepl('^"',trump$text), 1, 0) #if it begins with a quotation mark 1, if not then 0
#Filtering out non Trump or non staff language
trump_filter <- trump%>%
filter(quote==0)
#Feature 3: Contains a Link/ Picture
trump_filter$link <- ifelse(grepl("t.co",trump_filter$text), 1,0) #in the article he uses t.co
#Feature 4: Presence of a hastag
trump_filter$hashtag <- ifelse(grepl("#", trump_filter$text),1,0)
#Feature 5: Presence of an exclamation point at the end of the tweet
trump_filter$ExclamationEnd <- ifelse(grepl("!$", trump_filter$text),1,0)
#Feature 6,7,8,9: Does the tweet contain negative emotion? Does the tweet contain positive emotion?
#Number of positive words in the tweet? Number of negative emotions in the tweet?
#step one: We will tokenize each tweet - meaning that each word in a tweet will become its own row
#within this step we will take out links
#also this code takes out stop_words - According to Stiles and Robinson in their book: Text
#Mining with R - stop_words are  stop words are words that are not useful for an analysis,
#typically extremely common words such as “the”, “of”, “to”, and so forth in English.
reg <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))" #this reg ex. is taken from David Robinson's article
tweet_words <- trump_filter %>%
mutate(text = str_replace_all(text,  "https://t.co/[A-Za-z\\d]+|&amp;", "")) %>%
unnest_tokens(word, text, token = "regex", pattern = reg) %>%
filter(!word %in% stop_words$word,
str_detect(word, "[a-z]")) %>%
select(source, time_posted, word) #this code was adapted from David Robinson's article, as well
#Now we have all words - excluding stop words from each tweet, time_posted will serve as an ID
#step two: sentiment analysis- we will use the NRC Word Emotion Association Lexicon to see,
#which individual word in each tweet is as an emotion. The NRC is available in the tidy text package
#filter on 10 sentiments: positive, negative, anger, anticipation, disgust, fear, joy, sadness, suprise
# and trust.
nrc <- sentiments %>% #load in nrc words and sentiments
filter(lexicon == "nrc") %>%
dplyr::select(word, sentiment) %>%
filter(sentiment %in% c("trust", "positive", "joy", "fear", "negative", "sadness", "anger", "disgust", "anticipation",
"surprise"))
#step three: join nrc and tweet_words
Sentiments <- inner_join(tweet_words, nrc, by = "word")
#
positive_sent <- c("trust", "positive", "joy", "anticipation", "surprise")
negative_sent <- c("fear", "negative", "sadness", "anger", "disgust")
Sentiments$Positive <- ifelse(is.element(Sentiments$sentiment, positive_sent), 1, 0)
Sentiments$Negative <- ifelse(is.element(Sentiments$sentiment, negative_sent), 1, 0)
#IDs with positive emotions
PositiveIDs <- unique(Sentiments[Sentiments$Positive==1,2])
#IDs with negative emotions
NegativeIDs <- unique(Sentiments[Sentiments$Negative==1,2])
#Assigning Positive and Negative Emotions per Tweet
trump_filter$Positive <- ifelse(is.element(trump_filter$time_posted, PositiveIDs),1, 0)
trump_filter$Negative <- ifelse(is.element(trump_filter$time_posted, NegativeIDs),1, 0)
#Counts
SentimentsCount <- Sentiments %>%
group_by(source, time_posted) %>%
summarise(NumberPositiveEmotions = sum(Positive), NumberNegativeEmotions = sum(Negative))
#Assigning Counts of Positive and Negative Emotions per Tweet
trump_filter <- full_join(trump_filter, SentimentsCount)
#For counts, if 0 - then no emotion associated so we can make NAs 0
trump_filter<- trump_filter %>%
mutate(NumberPositiveEmotions = ifelse(is.na(NumberPositiveEmotions),0, NumberPositiveEmotions),
NumberNegativeEmotions = ifelse(is.na(NumberNegativeEmotions),0, NumberNegativeEmotions))
trump_filter %>%
group_by(source)%>%
summarise(PositiveEmotions = sum(NumberPositiveEmotions), NegativeEmotions = sum(NumberNegativeEmotions),
PercentNeg = NegativeEmotions/sum(NegativeEmotions+PositiveEmotions),
PercentPos = PositiveEmotions/sum(PositiveEmotions+NegativeEmotions))
#Feature 10: Presence of capitalized words (Check if 3 consecutive letters are capitalized)
##Omit if tweet contains link (because many links conta)
## Should be at least 4 to omit News organizations such as ABC, WSJ, etc.
trump_filter$capitalized <- ifelse(grepl("[A-Z]{4,}", trump_filter$text)== T &
grepl("t.co",trump_filter$text) ==F, 1, 0)
#Note sure how to exclude links with 4 consecutive letters
trump_filter[(grepl("[A-Z]{4,}", trump_filter$text)== T &
grepl("t.co",trump_filter$text) ==F),] %>% select(source, text) %>% group_by(source) %>%
summarize(n = n())
trump <- left_join(trump,trump_filter)
trump <-trump %>% mutate_all(function(x){ ifelse(is.na(x)==T, 0, x)})
str(trump)
#check
sum(is.na(trump))
rm(nrc,Sentiments, SentimentsCount, tweet_words)
# rename the label, make it a factor, and shuffle the data
trump <- trump %>% mutate(label = as.factor(source)) %>% select(-source) %>% slice(sample(1:n())) #shuffle data
#training dataset
split_size = .8*nrow(trump) #splits size  - we want it to be 80%
train <- trump %>% slice(1:split_size) #first half
train_labels <- train$label
train <- train %>% select(-label)#drops labels
#test dataset
test <- trump %>% slice(split_size+1:n()) #splits size - we want it to be 20%
test_labels <- test$label
test <- test %>% select(-label)
#Compare Number of Tweets by Hour
trump %>%
group_by(hour, label) %>%
summarize(number_of_tweets = n()) %>%
ggplot() +
geom_line(aes(x=hour, y=number_of_tweets, group = label, color = label))
ggplot(quote_summary, aes(x=label, y = n, fill = quote)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
#Compare Quotation Marks
(quote_summary <- trump %>%
mutate(quote = ifelse(quote==1, "quote", "no quote")) %>%
count(label, quote))
ggplot(quote_summary, aes(x=label, y = n, fill = quote)) +
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
(capital_summary <- trump %>%
mutate(all_caps = ifelse(capitalized==1, "All Caps", "No All Caps")) %>%
count(label, all_caps))
ggplot(capital_summary, aes(x=label, y=n, fill = all_caps))+
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
(links_summary <- trump %>%
mutate(link = ifelse(link==1, "Links", "No Link")) %>%
count(label, link))
ggplot(links_summary, aes(x=label, y=n, fill = link))+
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
(hashtag_summary <- trump %>%
mutate(hashtag = ifelse(hashtag==1, "Hashtag", "No Hashtag")) %>%
count(label, hashtag) )
ggplot(hashtag_summary, aes(x=label, y=n, fill = hashtag))+
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
(exclaim_summary <- trump %>%
mutate(exclamation_point = ifelse(ExclamationEnd==1, "Exclamation Point", "No Exclamation Point")) %>%
count(label, exclamation_point) )
ggplot(exclaim_summary, aes(x=label, y=n, fill = exclamation_point))+
geom_bar(stat = "identity", position = "dodge") +
labs(x = "", y = "Number of tweets", fill = "")
#### Part 5: Naive Bayes ####
#Function to Print Confusion Matrix and Calculate Accuracy
check_accuracy <- function(predictions, labels, phase){
confusion_matrix <- table(predictions, labels)
print(confusion_matrix)
accuracy <-  sum(diag(confusion_matrix )) / sum(confusion_matrix)
precision <-  confusion_matrix[2,2] / sum(confusion_matrix[2 , ] )
recall <-  confusion_matrix[2,2] / sum(confusion_matrix[ , 2] )
cat("Accuracy on", phase ,"Data: ", accuracy)
cat("\n Precision on", phase ,"Data: ", precision)
cat("\n Recall on", phase ,"Data: ", recall)
}
?naive_bayes
install.packages("naivebayes")
library(naivebayes)
?naive_bayes
#Full Model
#Create model and make predictions on training data
nb_mod <- naive_bayes(x=select(train, -text, -time_posted ), y=train_labels)
#Training Data
pred_train_1 <- predict(nb_mod, train)
check_accuracy(pred_train_1, train_labels, "Training")
338+528
338/866
338+13+113+528
866/992
528/(113+528)
528/(528+13)
pred_test_1 <- predict(nb_mod, test)
check_accuracy(pred_test_1, test_labels, "Test")
no_caps <- select(train, -text, -time_posted, -capitalized )
nb_mod2 <- naive_bayes(x=no_caps, y=train_labels)
pred_train_2<- predict(nb_mod2, train)
check_accuracy(pred_train_2, train_labels, "Training")
pred_test_2 <- predict(nb_mod2, test)
check_accuracy(pred_test_2, test_labels, "Test")
no_time <- select(train, -text, -time_posted, -capitalized, -hour)
nb_mod3 <- naive_bayes(x=no_time, y=train_labels)
nb_mod3$tables
#Training Data
pred_train_3 <- predict(nb_mod3, train)
check_accuracy(pred_train_3, train_labels, "Training")
#Test
pred_test_3 <- predict(nb_mod3, test)
check_accuracy(pred_test_3, test_labels, "Test")
#Model 4: Drop  Links
no_links <- select(train, -text, -time_posted, -capitalized, -hour, -link)
nb_mod4 <- naive_bayes(x=no_links, y=train_labels)
#Training Data
pred_train_4 <- predict(nb_mod4, train)
check_accuracy(pred_train_4, train_labels, "Training")
#Test
pred_test_4 <- predict(nb_mod4, test)
check_accuracy(pred_test_4, test_labels, "Test")
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive )
nb_mod5 <- naive_bayes(x=no_positive, y=train_labels )
#Model 5: Drop  Positive Emotions
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive )
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive )
#Model 5: Drop  Positive Emotions
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive)
View(train)
#Model 5: Drop  Positive Emotions
no_positive <- select(train, -text, -time_posted, -capitalized, -hour,
-NumberPositiveEmotions, -Positive)
nb_mod5 <- naive_bayes(x=no_positive, y=train_labels )
pred_train_5 <- predict(nb_mod5, train)
check_accuracy(pred_train_5, train_labels, "Training")
pred_test_5 <- predict(nb_mod5, test)
check_accuracy(pred_test_5, test_labels, "Training")
no_negative <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive, -Negative, - NumberNegativeEmotions )
nb_mod6 <- naive_bayes(x=no_negative, y=train_labels )
no_negative <- select(train, -text, -time_posted, -capitalized, -hour,
-NumberPositiveEmotions, -Positive, -Negative, - NumberNegativeEmotions )
nb_mod6 <- naive_bayes(x=no_negative, y=train_labels )
#Training Data
pred_train_6 <- predict(nb_mod6, train)
check_accuracy(pred_train_6, train_labels, "Training")
#Test Data
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
no_exclamation <- select(train, link, hashtag, quote)
nb_mod7 <- naive_bayes(x=no_exclamation, y=train_labels )
#Training Data
pred_train_7 <- predict(nb_mod7, train)
check_accuracy(pred_train_7, train_labels, "Training")
#Test Data
pred_test_7 <- predict(nb_mod7, test)
check_accuracy(pred_test_7, test_labels, "Test")
drop_hash <- select(train, quote,  link)
nb_mod8 <- naive_bayes(x=drop_hash, y=train_labels )
pred_train_8 <- predict(nb_mod8, train)
check_accuracy(pred_train_8, train_labels, "Training")
#Test Data
pred_test_8 <- predict(nb_mod8, test)
check_accuracy(pred_test_8, test_labels, "Test")
drop_quote <- select(train, quote, hashtag,  link)
nb_mod9 <- naive_bayes(x=drop_quote, y=train_labels )
pred_train_9 <- predict(nb_mod9, train)
check_accuracy(pred_train_9, train_labels, "Training")
#Test Data
pred_test_9 <- predict(nb_mod9, test)
check_accuracy(pred_test_9, test_labels, "Test")
#Model  With Link and Quote
nb_mod13 <- naive_bayes(x=select(train, link, quote), y=train_labels)
pred_train_13 <- predict(nb_mod13, train)
check_accuracy(pred_train_13, train_labels, "Training")
#Test Data
pred_test_13 <- predict(nb_mod13, test)
check_accuracy(pred_test_13, test_labels, "Test")
nb_mod11 <- naive_bayes(x=select(train, hashtag,  quote), y=train_labels)
pred_train_11 <- predict(nb_mod11, train)
check_accuracy(pred_train_11, train_labels, "Training")
#Test Data
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
nb_mod10 <- naive_bayes(x=select(train, link, hashtag), y=train_labels)
pred_train_10 <- predict(nb_mod10, train)
check_accuracy(pred_train_10, train_labels, "Training")
#Test Data
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
#Model 12: Link, Quote, and Hashtag
nb_mod12 <- naive_bayes(x=select(train, link, quote, hashtag), y=train_labels)
pred_train_12 <- predict(nb_mod12, train)
check_accuracy(pred_train_12, train_labels, "Training")
#Test Data
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
X <- naive_bayes(x=train, y = train_labels)
Xpred <- predict(X, train)
check_accuracy(Xpredm train_labels, "Training")
check_accuracy(Xpred, train_labels, "Training")
Xpred <- predict(X, test )
Xpred <- predict(X, test)
X <- naive_bayes(x=train,y=train_labels)
Labs <- predict(X, test)
