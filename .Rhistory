check_accuracy(pred_train_11, train_labels, "Training")
#Test Data
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
#Best Model
nb_mod11 <- naive_bayes(x=select(train, hashtag,  quote), y=train_labels)
pred_train_11 <- predict(nb_mod11, train)
check_accuracy(pred_train_11, train_labels, "Training")
#Test Data
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
table(trump$label, trump$link)
#Best Model
nb_mod10 <- naive_bayes(x=select(train, link, hashtag), y=train_labels)
pred_train_10 <- predict(nb_mod10, train)
check_accuracy(pred_train_10, train_labels, "Training")
#Test Data
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
#Link, Quote, and Hashtag
nb_mod12 <- naive_bayes(x=select(train, link, quote, hashtag), y=train_labels)
pred_train_12 <- predict(nb_mod12, train)
check_accuracy(pred_train_12, train_labels, "Training")
#Test Data
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
nb_mod12
nb_mod10
nb_mod12
#Function to Print Confusion Matrix and Calculate Accuracy
check_accuracy <- function(predictions, labels, phase){
confusion_matrix <- table(predictions, labels)
print(confusion_matrix)
accuracy <-  sum(diag(confusion_matrix )) / sum(confusion_matrix)
precision <-  confusion_matrix[2,2] / sum(confusion_matrix[2 , ] )
recall <-  confusion_matrix[2,2] / sum(confusion_matrix[ , 2] )
cat("Accuracy on", phase ,"Data: ", accuracy)
cat("\n Precision on", phase ,"Data: ", precision)
cat("\n Recall on", phase ,"Data: ", recall)
}
#Full Model
#Create model and make predictions on training data
nb_mod <- naive_bayes(x=select(train, -text, -time_posted ), y=train_labels)
#Training Data
pred_train_1 <- predict(nb_mod, train)
check_accuracy(pred_train_1, train_labels, "Training")
#Test
pred_test_1 <- predict(nb_mod, test)
check_accuracy(pred_test_1, test_labels, "Test")
#Model 12: Link, Quote, and Hashtag
nb_mod12 <- naive_bayes(x=select(train, link, quote, hashtag), y=train_labels)
pred_train_12 <- predict(nb_mod12, train)
check_accuracy(pred_train_12, train_labels, "Training")
#Test Data
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
#Model 10: Link and Hashtag
nb_mod10 <- naive_bayes(x=select(train, link, hashtag), y=train_labels)
pred_train_10 <- predict(nb_mod10, train)
check_accuracy(pred_train_10, train_labels, "Training")
#Test Data
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
#Model with Hashtag and Quote
nb_mod11 <- naive_bayes(x=select(train, hashtag,  quote), y=train_labels)
pred_train_11 <- predict(nb_mod11, train)
check_accuracy(pred_train_11, train_labels, "Training")
#Test Data
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
#Model  With Link and Quote
nb_mod13 <- naive_bayes(x=select(train, link, quote), y=train_labels)
pred_train_13 <- predict(nb_mod13, train)
check_accuracy(pred_train_13, train_labels, "Training")
#Test Data
pred_test_13 <- predict(nb_mod11, test)
check_accuracy(pred_test_13, test_labels, "Test")
#Model 9: Drop Quote
drop_quote <- select(train, quote, hashtag,  link)
nb_mod9 <- naive_bayes(x=drop_quote, y=train_labels )
pred_train_9 <- predict(nb_mod9, train)
check_accuracy(pred_train_9, train_labels, "Training")
#Test Data
pred_test_9 <- predict(nb_mod9, test)
check_accuracy(pred_test_9, test_labels, "Test")
#Model 8: Drop Hashtag
drop_hash <- select(train, quote,  link)
nb_mod8 <- naive_bayes(x=drop_hash, y=train_labels )
pred_train_8 <- predict(nb_mod8, train)
check_accuracy(pred_train_8, train_labels, "Training")
#Test Data
pred_test_8 <- predict(nb_mod8, test)
check_accuracy(pred_test_8, test_labels, "Test")
'It looks like we should keep hashtag. '
#Model 7: Drop Exclamation Point
no_exclamation <- select(train, link, hashtag, quote)
nb_mod7 <- naive_bayes(x=no_exclamation, y=train_labels )
#Training Data
pred_train_7 <- predict(nb_mod7, train)
check_accuracy(pred_train_7, train_labels, "Training")
#Test Data
pred_test_7 <- predict(nb_mod7, test)
check_accuracy(pred_test_7, test_labels, "Test")
#Model 6: Drop  Negative Emotions
no_negative <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive, -Negative, - NumberNegativeEmotions )
nb_mod6 <- naive_bayes(x=no_negative, y=train_labels )
#Training Data
pred_train_6 <- predict(nb_mod6, train)
check_accuracy(pred_train_6, train_labels, "Training")
#Test Data
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
#Model 5: Drop  Positive Emotions
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive )
nb_mod5 <- naive_bayes(x=no_positive, y=train_labels )
#Training Data
pred_train_5 <- predict(nb_mod5, train)
check_accuracy(pred_train_5, train_labels, "Training")
#Test Data
pred_test_5 <- predict(nb_mod5, test)
check_accuracy(pred_test_5, test_labels, "Training")
#Model 4: Drop  Links
no_links <- select(train, -text, -time_posted, -capitalized, -hour, -link)
nb_mod4 <- naive_bayes(x=no_links, y=train_labels)
#Training Data
pred_train_4 <- predict(nb_mod4, train)
check_accuracy(pred_train_4, train_labels, "Training")
#Test
pred_test_4 <- predict(nb_mod4, test)
check_accuracy(pred_test_4, test_labels, "Test")
'
It looks like we should definitely keep links.
'
#Model 3: Drop Time
no_time <- select(train, -text, -time_posted, -capitalized, -hour)
nb_mod3 <- naive_bayes(x=no_time, y=train_labels)
nb_mod3$tables
#Training Data
pred_train_3 <- predict(nb_mod3, train)
check_accuracy(pred_train_3, train_labels, "Training")
#Test
pred_test_3 <- predict(nb_mod3, test)
check_accuracy(pred_test_3, test_labels, "Test")
#Model 2: Drop All Caps Field
no_caps <- select(train, -text, -time_posted, -capitalized )
nb_mod2 <- naive_bayes(x=no_caps, y=train_labels)
#Training Data
pred_train_2<- predict(nb_mod2, train)
check_accuracy(pred_train_2, train_labels, "Training")
#Test
pred_test_2 <- predict(nb_mod2, test)
check_accuracy(pred_test_2, test_labels, "Test")
#Full Model
#Create model and make predictions on training data
nb_mod <- naive_bayes(x=select(train, -text, -time_posted ), y=train_labels)
#Training Data
pred_train_1 <- predict(nb_mod, train)
check_accuracy(pred_train_1, train_labels, "Training")
#Test
pred_test_1 <- predict(nb_mod, test)
check_accuracy(pred_test_1, test_labels, "Test")
#Model 2: Drop All Caps Field
no_caps <- select(train, -text, -time_posted, -capitalized )
nb_mod2 <- naive_bayes(x=no_caps, y=train_labels)
#Training Data
pred_train_2<- predict(nb_mod2, train)
check_accuracy(pred_train_2, train_labels, "Training")
#Test
pred_test_2 <- predict(nb_mod2, test)
check_accuracy(pred_test_2, test_labels, "Test")
#Model 3: Drop Time
no_time <- select(train, -text, -time_posted, -capitalized, -hour)
nb_mod3 <- naive_bayes(x=no_time, y=train_labels)
nb_mod3$tables
#Training Data
pred_train_3 <- predict(nb_mod3, train)
check_accuracy(pred_train_3, train_labels, "Training")
#Test
pred_test_3 <- predict(nb_mod3, test)
check_accuracy(pred_test_3, test_labels, "Test")
eck_accuracy(pred_test_2, test_labels, "Test")
#Model 3: Drop Time
no_time <- select(train, -text, -time_posted, -capitalized, -hour)
nb_mod3 <- naive_bayes(x=no_time, y=train_labels)
nb_mod3$tables
#Training Data
pred_train_3 <- predict(nb_mod3, train)
check_accuracy(pred_train_3, train_labels, "Training")
#Test
pred_test_3 <- predict(nb_mod3, test)
check_accuracy(pred_test_3, test_labels, "Test")
#Model 4: Drop  Links
no_links <- select(train, -text, -time_posted, -capitalized, -hour, -link)
nb_mod4 <- naive_bayes(x=no_links, y=train_labels)
#Training Data
pred_train_4 <- predict(nb_mod4, train)
check_accuracy(pred_train_4, train_labels, "Training")
#Test
pred_test_4 <- predict(nb_mod4, test)
check_accuracy(pred_test_4, test_labels, "Test")
'
It looks like we should definitely keep links.
'
#Model 5: Drop  Positive Emotions
no_positive <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive )
nb_mod5 <- naive_bayes(x=no_positive, y=train_labels )
#Training Data
pred_train_5 <- predict(nb_mod5, train)
check_accuracy(pred_train_5, train_labels, "Training")
#Test Data
pred_test_5 <- predict(nb_mod5, test)
check_accuracy(pred_test_5, test_labels, "Training")
#Model 6: Drop  Negative Emotions
no_negative <- select(train, -text, -time_posted, -capitalized, -hour
-NumberPositiveEmotions, -Positive, -Negative, - NumberNegativeEmotions )
nb_mod6 <- naive_bayes(x=no_negative, y=train_labels )
#Training Data
pred_train_6 <- predict(nb_mod6, train)
check_accuracy(pred_train_6, train_labels, "Training")
#Test Data
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
#Model 7: Drop Exclamation Point
no_exclamation <- select(train, link, hashtag, quote)
nb_mod7 <- naive_bayes(x=no_exclamation, y=train_labels )
#Training Data
pred_train_7 <- predict(nb_mod7, train)
check_accuracy(pred_train_7, train_labels, "Training")
#Test Data
pred_test_7 <- predict(nb_mod7, test)
check_accuracy(pred_test_7, test_labels, "Test")
#Model 8: Drop Hashtag
drop_hash <- select(train, quote,  link)
nb_mod8 <- naive_bayes(x=drop_hash, y=train_labels )
pred_train_8 <- predict(nb_mod8, train)
check_accuracy(pred_train_8, train_labels, "Training")
#Test Data
pred_test_8 <- predict(nb_mod8, test)
check_accuracy(pred_test_8, test_labels, "Test")
'It looks like we should keep hashtag. '
#Model 9: Drop Quote
drop_quote <- select(train, quote, hashtag,  link)
nb_mod9 <- naive_bayes(x=drop_quote, y=train_labels )
pred_train_9 <- predict(nb_mod9, train)
check_accuracy(pred_train_9, train_labels, "Training")
#Test Data
pred_test_9 <- predict(nb_mod9, test)
check_accuracy(pred_test_9, test_labels, "Test")
#Model  With Link and Quote
nb_mod13 <- naive_bayes(x=select(train, link, quote), y=train_labels)
pred_train_13 <- predict(nb_mod13, train)
check_accuracy(pred_train_13, train_labels, "Training")
#Test Data
pred_test_13 <- predict(nb_mod11, test)
check_accuracy(pred_test_13, test_labels, "Test")
#Model with Hashtag and Quote
nb_mod11 <- naive_bayes(x=select(train, hashtag,  quote), y=train_labels)
pred_train_11 <- predict(nb_mod11, train)
check_accuracy(pred_train_11, train_labels, "Training")
#Test Data
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
#Best Models
#Best Models: 10 & 12
#Model 10: Link and Hashtag
nb_mod10 <- naive_bayes(x=select(train, link, hashtag), y=train_labels)
pred_train_10 <- predict(nb_mod10, train)
check_accuracy(pred_train_10, train_labels, "Training")
#Test Data
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
#Model 12: Link, Quote, and Hashtag
nb_mod12 <- naive_bayes(x=select(train, link, quote, hashtag), y=train_labels)
pred_train_12 <- predict(nb_mod12, train)
check_accuracy(pred_train_12, train_labels, "Training")
#Test Data
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
#Model 12: Link, Quote, and Hashtag
nb_mod12 <- naive_bayes(x=select(train, link, quote, hashtag), y=train_labels)
pred_train_12 <- predict(nb_mod12, train)
check_accuracy(pred_train_12, train_labels, "Training")
#Test Data
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
check_accuracy(pred_test_12, test_labels, "Test")
check_accuracy(pred_test_12, test_labels, "Test")
check_accuracy(pred_test_1, test_labels, "Test")
check_accuracy(pred_test_1, test_labels, "Test")
check_accuracy(pred_test_1, test_labels, "Test")
check_accuracy(pred_test_2, test_labels, "Test")
check_accuracy(pred_test_3, test_labels, "Test")
check_accuracy(pred_test_2, test_labels, "Test")
check_accuracy(pred_test_4, test_labels, "Test")
check_accuracy(pred_test_5, test_labels, "Training")
pred_test_6 <- predict(nb_mod6, test)
check_accuracy(pred_test_6, test_labels, "Test")
check_accuracy(pred_test_7, test_labels, "Test")
check_accuracy(pred_test_8, test_labels, "Test")
check_accuracy(pred_test_9, test_labels, "Test")
check_accuracy(pred_test_13, test_labels, "Test")
pred_test_13 <- predict(nb_mod13, test)
check_accuracy(pred_test_13, test_labels, "Test")
pred_test_11 <- predict(nb_mod11, test)
check_accuracy(pred_test_11, test_labels, "Test")
pred_test_10 <- predict(nb_mod10, test)
check_accuracy(pred_test_10, test_labels, "Test")
pred_test_12 <- predict(nb_mod12, test)
check_accuracy(pred_test_12, test_labels, "Test")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table)
poll<-as.data.frame(fread(file.choose()))
poll <- data.frame(apply(poll,2,function(x) factor(x)))
poll$vote_2008 <- relevel(poll$vote_2008, ref = "john mcCain")
levels(poll$vote_2008)
summary(glm(vote_2008~.,data = poll, family = "binomial"))
#probabilistic predictions into binary predictions
f1 <- glm(vote_2008~.,data = poll, family = "binomial")
predicted <- predict(f1, type = "response")
binarypredictions <- ifelse(predicted > .5, "Obama", "McCain")
table(binarypredictions)
#accuracy
confusion_matrix <- table(binarypredictions, poll$vote_2008)
(confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)
#precision
(5119)/(823+5119)
#recall
5119/(5119+600)
binarypredictions_new <- ifelse(predicted > .7, "Obama", "McCain")
table(binarypredictions_new)
#accuracy
confusion_matrix_new <- table(binarypredictions_new, poll$vote_2008)
(confusion_matrix_new[1,1]+confusion_matrix_new[2,2])/sum(confusion_matrix)
#precision
(4410)/(4410+398)
#recall
(4410)/(4410+1309)
#read in data
poll_full <- as.data.frame(fread(file.choose()))
#mutate with if_else to make democrat + republican =1 for major party, other = 0
poll_full$MajorParty <- ifelse(poll_full$vote_2008 == "other", 0, 1)
#factor
poll_full$MajorParty <- factor(poll_full$MajorParty)
library(MASS)
bigmodel <- glm(MajorParty~.-vote_2008, data = poll_full, family = "binomial")
# summary(stepAIC(bigmodel, scope = list(lower = ~1, upper = ~ .^2), direction = "both"))
# finalmodel <-glm(formula = MajorParty ~ sex + race + party + ideology + party:ideology +
#     race:party + sex:race + sex:party, family = "binomial", data = poll_full)
summary(finalmodel)
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=predicted2))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=prob_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
#filtering
library(tidyverse)
poll_full_filter <- poll_full %>%
filter(MajorParty == 1)%>%
mutate(vote_2008 = factor(vote_2008))%>%
select(-MajorParty)
poll_full_filter <- data.frame(apply(poll_full_filter,2,function(x) factor(x)))
poll_full_filter$vote_2008 <- relevel(poll_full_filter$vote_2008, ref = "john mcCain")
levels(poll_full_filter$vote_2008)
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities")+
labs(x="probabilities")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Obama | Voted for Major Party")+
labs(x="probabilities")
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(data.table)
poll<-as.data.frame(fread(file.choose()))
poll <- data.frame(apply(poll,2,function(x) factor(x)))
poll$vote_2008 <- relevel(poll$vote_2008, ref = "john mcCain")
levels(poll$vote_2008)
summary(glm(vote_2008~.,data = poll, family = "binomial"))
#probabilistic predictions into binary predictions
f1 <- glm(vote_2008~.,data = poll, family = "binomial")
predicted <- predict(f1, type = "response")
binarypredictions <- ifelse(predicted > .5, "Obama", "McCain")
table(binarypredictions)
#accuracy
confusion_matrix <- table(binarypredictions, poll$vote_2008)
(confusion_matrix[1,1]+confusion_matrix[2,2])/sum(confusion_matrix)
#precision
(5119)/(823+5119)
#recall
5119/(5119+600)
binarypredictions_new <- ifelse(predicted > .7, "Obama", "McCain")
table(binarypredictions_new)
#accuracy
confusion_matrix_new <- table(binarypredictions_new, poll$vote_2008)
(confusion_matrix_new[1,1]+confusion_matrix_new[2,2])/sum(confusion_matrix)
#precision
(4410)/(4410+398)
#recall
(4410)/(4410+1309)
#read in data
poll_full <- as.data.frame(fread(file.choose()))
#mutate with if_else to make democrat + republican =1 for major party, other = 0
poll_full$MajorParty <- ifelse(poll_full$vote_2008 == "other", 0, 1)
#factor
poll_full$MajorParty <- factor(poll_full$MajorParty)
library(MASS)
bigmodel <- glm(MajorParty~.-vote_2008, data = poll_full, family = "binomial")
# summary(stepAIC(bigmodel, scope = list(lower = ~1, upper = ~ .^2), direction = "both"))
# finalmodel <-glm(formula = MajorParty ~ sex + race + party + ideology + party:ideology +
#     race:party + sex:race + sex:party, family = "binomial", data = poll_full)
#summary(finalmodel)
# #Make a histogram of the resulting predicted probabilities using ggplot2
# predicted <- predict(finalmodel, type = "response")
# predicted <- data.frame(predicted)
#
# #histogram
# library(ggplot2)
# ggplot(data=predicted,aes(x=predicted))+
#   geom_histogram()+
#   ggtitle("Histogram of Predicted Probabilities")
prob_major_party <- predict(bigmodel,type="response")
prob_major_party <- data.frame(prob_major_party)
ggplot(data=prob_major_party, aes(x=prob_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Major Party")+
labs(x="probabilities")
#filtering
library(tidyverse)
poll_full_filter <- poll_full %>%
filter(MajorParty == 1)%>%
mutate(vote_2008 = factor(vote_2008))%>%
select(-MajorParty)
poll_full_filter <- data.frame(apply(poll_full_filter,2,function(x) factor(x)))
poll_full_filter$vote_2008 <- relevel(poll_full_filter$vote_2008, ref = "john mcCain")
levels(poll_full_filter$vote_2008)
filter_mod <- glm(vote_2008 ~., data = poll_full_filter, family = "binomial")
#Generate Estimates for EVERY individual
prob_obama_given_major_party <- predict(filter_mod, type = "response", newdata = poll_full )
prob_obama_given_major_party<- data.frame(prob_obama_given_major_party)
ggplot(data=prob_obama_given_major_party, aes(x=prob_obama_given_major_party))+
geom_histogram(color = "black", fill = "white")+
ggtitle("Histogram of Predicted Probabilities of Voting for Obama | Voted for Major Party")+
labs(x="probabilities")
#Prob(Obama) = Prob(Obama|Voted for Major Party)*Prob(Major Party)
prob_obama <- prob_obama_given_major_party*prob_major_party
#Prob(McCain) = Prob(McCain|Voted for Major Party)*Prob(Major Party)
#=(1- Prob(Obama|Voted for Major Party)*Prob(Major Party)
prob_mccain <- (1 - prob_obama_given_major_party)*prob_major_party
#Prob(Other) = 1- Prob(Major Party)
prob_other <- (1- prob_major_party)
#Check that everything adds up to 1
mean(round(prob_obama+prob_mccain+prob_other, 4) == 1)
#Bind Probabilities into Dataframe
probabilities <- data.frame(prob_obama = prob_obama,
prob_mccain = prob_mccain,
prob_other = prob_other)
#Generate new variable of predicted vote for each individual
#Predicted vote is based on highest probability
probabilities <- probabilities %>%
mutate(predicted_vote = as.factor(case_when(
prob_obama > prob_mccain &  prob_obama > prob_other ~ "barack obama",
prob_mccain > prob_obama & prob_mccain > prob_other ~ "john mcCain",
prob_other > prob_obama & prob_other > prob_mccain ~ "other"
)))
#Add "other" to factor levels since there were no "other" predictions
levels(probabilities$predicted_vote) <- c("barack obama", "john mcCain", "other")
#Explore Vote Predictions
table(probabilities$predicted_vote)
#Confusion Matrix
confusion_mat_3 <- table(probabilities$predicted_vote, poll_full$vote_2008)
confusion_mat_3
#Compute Accuracy
cat("\n Accuracy: ", sum(diag(confusion_mat_3)) / sum(confusion_mat_3))
#84% accuracy; not too bad
